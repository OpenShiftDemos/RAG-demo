{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "016b5598",
   "metadata": {
    "id": "016b5598"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/vector_stores/RedisIndexDemo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b692c73",
   "metadata": {
    "id": "0b692c73"
   },
   "source": [
    "# Redis Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7787c2",
   "metadata": {
    "id": "1e7787c2"
   },
   "source": [
    "In this notebook we are going to show a quick demo of using the RedisVectorStore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c479ce87",
   "metadata": {
    "id": "c479ce87"
   },
   "source": [
    "If you're opening this Notebook on colab, you will probably need to install LlamaIndex 🦙."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1730d643",
   "metadata": {
    "collapsed": true,
    "id": "1730d643",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-vector-stores-redis\n",
      "  Downloading llama_index_vector_stores_redis-0.1.2-py3-none-any.whl (8.0 kB)\n",
      "Collecting redis<6.0.0,>=5.0.1\n",
      "  Downloading redis-5.0.2-py3-none-any.whl (251 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.7/251.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting llama-index-core<0.11.0,>=0.10.1\n",
      "  Downloading llama_index_core-0.10.14-py3-none-any.whl (15.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/app-root/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-redis) (1.24.4)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/app-root/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-redis) (1.6.0)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /opt/app-root/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-redis) (1.2.14)\n",
      "Requirement already satisfied: pandas in /opt/app-root/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-redis) (1.5.3)\n",
      "Collecting httpx\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m239.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting SQLAlchemy[asyncio]>=1.4.49\n",
      "  Downloading SQLAlchemy-2.0.27-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=6.0.1 in /opt/app-root/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-redis) (6.0.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /opt/app-root/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-redis) (3.9.1)\n",
      "Requirement already satisfied: networkx>=3.0 in /opt/app-root/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-redis) (3.2.1)\n",
      "Collecting tiktoken>=0.3.3\n",
      "  Downloading tiktoken-0.6.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting openai>=1.1.0\n",
      "  Downloading openai-1.13.3-py3-none-any.whl (227 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m278.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting llamaindex-py-client<0.2.0,>=0.1.13\n",
      "  Downloading llamaindex_py_client-0.1.13-py3-none-any.whl (107 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m272.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-inspect>=0.8.0 in /opt/app-root/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-redis) (0.9.0)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /opt/app-root/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-redis) (10.2.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in /opt/app-root/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-redis) (2.31.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /opt/app-root/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-redis) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/app-root/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-redis) (2023.12.2)\n",
      "Collecting nltk<4.0.0,>=3.8.1\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting dataclasses-json\n",
      "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /opt/app-root/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-redis) (8.2.3)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8\n",
      "  Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/app-root/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-redis) (4.9.0)\n",
      "Requirement already satisfied: async-timeout>=4.0.3 in /opt/app-root/lib/python3.9/site-packages (from redis<6.0.0,>=5.0.1->llama-index-vector-stores-redis) (4.0.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/app-root/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-redis) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-redis) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/app-root/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-redis) (1.4.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-redis) (23.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/app-root/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-redis) (1.3.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/app-root/lib/python3.9/site-packages (from deprecated>=1.2.9.3->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-redis) (1.16.0)\n",
      "Requirement already satisfied: pydantic>=1.10 in /opt/app-root/lib/python3.9/site-packages (from llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-redis) (1.10.14)\n",
      "Requirement already satisfied: certifi in /opt/app-root/lib/python3.9/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-redis) (2023.11.17)\n",
      "Requirement already satisfied: sniffio in /opt/app-root/lib/python3.9/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-redis) (1.3.0)\n",
      "Requirement already satisfied: idna in /opt/app-root/lib/python3.9/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-redis) (3.6)\n",
      "Collecting httpcore==1.*\n",
      "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m256.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: anyio in /opt/app-root/lib/python3.9/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-redis) (4.2.0)\n",
      "Collecting h11<0.15,>=0.13\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m248.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib in /opt/app-root/lib/python3.9/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-redis) (1.3.2)\n",
      "Requirement already satisfied: click in /opt/app-root/lib/python3.9/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-redis) (8.1.7)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2023.12.25-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m773.4/773.4 kB\u001b[0m \u001b[31m208.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting distro<2,>=1.7.0\n",
      "  Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib/python3.9/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-redis) (1.26.18)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib/python3.9/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-redis) (3.3.2)\n",
      "Collecting greenlet!=0.4.17\n",
      "  Downloading greenlet-3.0.3-cp39-cp39-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (614 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m614.3/614.3 kB\u001b[0m \u001b[31m244.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/app-root/lib/python3.9/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-redis) (1.0.0)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0\n",
      "  Downloading marshmallow-3.21.0-py3-none-any.whl (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m197.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /opt/app-root/lib/python3.9/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-redis) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/app-root/lib/python3.9/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-redis) (2023.3.post1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/app-root/lib/python3.9/site-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-redis) (1.2.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/app-root/lib/python3.9/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-redis) (23.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-redis) (1.16.0)\n",
      "Installing collected packages: dirtyjson, regex, redis, marshmallow, h11, greenlet, distro, tiktoken, SQLAlchemy, nltk, httpcore, dataclasses-json, httpx, openai, llamaindex-py-client, llama-index-core, llama-index-vector-stores-redis\n",
      "Successfully installed SQLAlchemy-2.0.27 dataclasses-json-0.6.4 dirtyjson-1.0.8 distro-1.9.0 greenlet-3.0.3 h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 llama-index-core-0.10.14 llama-index-vector-stores-redis-0.1.2 llamaindex-py-client-0.1.13 marshmallow-3.21.0 nltk-3.8.1 openai-1.13.3 redis-5.0.2 regex-2023.12.25 tiktoken-0.6.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index-vector-stores-redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c71a947d",
   "metadata": {
    "collapsed": true,
    "id": "c71a947d",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index\n",
      "  Downloading llama_index-0.10.14-py3-none-any.whl (5.6 kB)\n",
      "Collecting llama-index-embeddings-openai<0.2.0,>=0.1.5\n",
      "  Downloading llama_index_embeddings_openai-0.1.6-py3-none-any.whl (6.0 kB)\n",
      "Collecting llama-index-llms-openai<0.2.0,>=0.1.5\n",
      "  Downloading llama_index_llms_openai-0.1.7-py3-none-any.whl (9.3 kB)\n",
      "Collecting llama-index-agent-openai<0.2.0,>=0.1.4\n",
      "  Downloading llama_index_agent_openai-0.1.5-py3-none-any.whl (12 kB)\n",
      "Collecting llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.1.3-py3-none-any.whl (6.6 kB)\n",
      "Collecting llama-index-readers-llama-parse<0.2.0,>=0.1.2\n",
      "  Downloading llama_index_readers_llama_parse-0.1.3-py3-none-any.whl (2.5 kB)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.14 in /opt/app-root/lib/python3.9/site-packages (from llama-index) (0.10.14)\n",
      "Collecting llama-index-cli<0.2.0,>=0.1.2\n",
      "  Downloading llama_index_cli-0.1.6-py3-none-any.whl (25 kB)\n",
      "Collecting llama-index-readers-file<0.2.0,>=0.1.4\n",
      "  Downloading llama_index_readers_file-0.1.6-py3-none-any.whl (34 kB)\n",
      "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3\n",
      "  Downloading llama_index_multi_modal_llms_openai-0.1.4-py3-none-any.whl (5.8 kB)\n",
      "Collecting llama-index-legacy<0.10.0,>=0.9.48\n",
      "  Downloading llama_index_legacy-0.9.48-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting llama-index-program-openai<0.2.0,>=0.1.3\n",
      "  Downloading llama_index_program_openai-0.1.4-py3-none-any.whl (4.1 kB)\n",
      "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2\n",
      "  Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\n",
      "Collecting llama-index-vector-stores-chroma<0.2.0,>=0.1.1\n",
      "  Downloading llama_index_vector_stores_chroma-0.1.5-py3-none-any.whl (4.7 kB)\n",
      "Requirement already satisfied: networkx>=3.0 in /opt/app-root/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.14->llama-index) (3.2.1)\n",
      "Requirement already satisfied: requests>=2.31.0 in /opt/app-root/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.14->llama-index) (2.31.0)\n",
      "Requirement already satisfied: dataclasses-json in /opt/app-root/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.14->llama-index) (0.6.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /opt/app-root/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.14->llama-index) (10.2.0)\n",
      "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.13 in /opt/app-root/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.14->llama-index) (0.1.13)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /opt/app-root/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.14->llama-index) (3.8.1)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /opt/app-root/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.14->llama-index) (0.6.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/app-root/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.14->llama-index) (2023.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /opt/app-root/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.14->llama-index) (0.9.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/app-root/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.14->llama-index) (1.6.0)\n",
      "Requirement already satisfied: httpx in /opt/app-root/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.14->llama-index) (0.27.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /opt/app-root/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.14->llama-index) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/app-root/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.14->llama-index) (4.9.0)\n",
      "Requirement already satisfied: openai>=1.1.0 in /opt/app-root/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.14->llama-index) (1.13.3)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /opt/app-root/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.14->llama-index) (1.2.14)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /opt/app-root/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.14->llama-index) (6.0.1)\n",
      "Requirement already satisfied: numpy in /opt/app-root/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.14->llama-index) (1.24.4)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /opt/app-root/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.14->llama-index) (1.0.8)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /opt/app-root/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.14->llama-index) (3.9.1)\n",
      "Requirement already satisfied: pandas in /opt/app-root/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.14->llama-index) (1.5.3)\n",
      "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /opt/app-root/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.14->llama-index) (2.0.27)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /opt/app-root/lib/python3.9/site-packages (from llama-index-core<0.11.0,>=0.10.14->llama-index) (8.2.3)\n",
      "Collecting pymupdf<2.0.0,>=1.23.21\n",
      "  Downloading PyMuPDF-1.23.26-cp39-none-manylinux2014_x86_64.whl (4.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m168.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting bs4<0.0.3,>=0.0.2\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Collecting pypdf<5.0.0,>=4.0.1\n",
      "  Downloading pypdf-4.0.2-py3-none-any.whl (283 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.0/284.0 kB\u001b[0m \u001b[31m303.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /opt/app-root/lib/python3.9/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (4.12.3)\n",
      "Collecting llama-parse<0.4.0,>=0.3.3\n",
      "  Downloading llama_parse-0.3.5-py3-none-any.whl (7.7 kB)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/app-root/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.14->llama-index) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/app-root/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.14->llama-index) (1.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.14->llama-index) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.14->llama-index) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.14->llama-index) (23.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/app-root/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.14->llama-index) (1.3.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/app-root/lib/python3.9/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (2.5)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/app-root/lib/python3.9/site-packages (from deprecated>=1.2.9.3->llama-index-core<0.11.0,>=0.10.14->llama-index) (1.16.0)\n",
      "Collecting chromadb<0.5.0,>=0.4.22\n",
      "  Downloading chromadb-0.4.24-py3-none-any.whl (525 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m525.5/525.5 kB\u001b[0m \u001b[31m306.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting onnxruntime<2.0.0,>=1.17.0\n",
      "  Downloading onnxruntime-1.17.1-cp39-cp39-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m154.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<0.16.0,>=0.15.1\n",
      "  Downloading tokenizers-0.15.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m160.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pydantic>=1.10 in /opt/app-root/lib/python3.9/site-packages (from llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.14->llama-index) (1.10.14)\n",
      "Requirement already satisfied: sniffio in /opt/app-root/lib/python3.9/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.14->llama-index) (1.3.0)\n",
      "Requirement already satisfied: idna in /opt/app-root/lib/python3.9/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.14->llama-index) (3.6)\n",
      "Requirement already satisfied: certifi in /opt/app-root/lib/python3.9/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.14->llama-index) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/app-root/lib/python3.9/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.14->llama-index) (1.0.4)\n",
      "Requirement already satisfied: anyio in /opt/app-root/lib/python3.9/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.14->llama-index) (4.2.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/app-root/lib/python3.9/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.14->llama-index) (0.14.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/app-root/lib/python3.9/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.14->llama-index) (2023.12.25)\n",
      "Requirement already satisfied: click in /opt/app-root/lib/python3.9/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.14->llama-index) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/app-root/lib/python3.9/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.14->llama-index) (1.3.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/app-root/lib/python3.9/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.14->llama-index) (1.9.0)\n",
      "Collecting PyMuPDFb==1.23.22\n",
      "  Downloading PyMuPDFb-1.23.22-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m208.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib/python3.9/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.14->llama-index) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib/python3.9/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.14->llama-index) (1.26.18)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/app-root/lib/python3.9/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.14->llama-index) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/app-root/lib/python3.9/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.14->llama-index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/app-root/lib/python3.9/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.14->llama-index) (3.21.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/app-root/lib/python3.9/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.14->llama-index) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/app-root/lib/python3.9/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.14->llama-index) (2023.3.post1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/app-root/lib/python3.9/site-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.14->llama-index) (1.2.0)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /opt/app-root/lib/python3.9/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.60.0)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /opt/app-root/lib/python3.9/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (7.6.0)\n",
      "Collecting pulsar-client>=3.1.0\n",
      "  Downloading pulsar_client-3.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m223.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting mmh3>=4.0.1\n",
      "  Downloading mmh3-4.1.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.4/67.4 kB\u001b[0m \u001b[31m211.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting uvicorn[standard]>=0.18.3\n",
      "  Downloading uvicorn-0.27.1-py3-none-any.whl (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m235.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typer>=0.9.0 in /opt/app-root/lib/python3.9/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.9.0)\n",
      "Collecting build>=1.0.3\n",
      "  Downloading build-1.1.1-py3-none-any.whl (19 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.23.0-py3-none-any.whl (18 kB)\n",
      "Collecting importlib-resources\n",
      "  Downloading importlib_resources-6.1.2-py3-none-any.whl (34 kB)\n",
      "Collecting opentelemetry-api>=1.2.0\n",
      "  Downloading opentelemetry_api-1.23.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m242.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pypika>=0.48.9\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m249.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting opentelemetry-sdk>=1.2.0\n",
      "  Downloading opentelemetry_sdk-1.23.0-py3-none-any.whl (105 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.7/105.7 kB\u001b[0m \u001b[31m274.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting posthog>=2.4.0\n",
      "  Downloading posthog-3.4.2-py2.py3-none-any.whl (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 kB\u001b[0m \u001b[31m213.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting opentelemetry-instrumentation-fastapi>=0.41b0\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.44b0-py3-none-any.whl (11 kB)\n",
      "Collecting kubernetes>=28.1.0\n",
      "  Downloading kubernetes-29.0.0-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m324.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting chroma-hnswlib==0.7.3\n",
      "  Downloading chroma_hnswlib-0.7.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m326.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting orjson>=3.9.12\n",
      "  Downloading orjson-3.9.15-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.4/138.4 kB\u001b[0m \u001b[31m286.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: bcrypt>=4.0.1 in /opt/app-root/lib/python3.9/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (4.1.2)\n",
      "Collecting fastapi>=0.95.2\n",
      "  Downloading fastapi-0.110.0-py3-none-any.whl (92 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m253.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=17.0 in /opt/app-root/lib/python3.9/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.14->llama-index) (23.2)\n",
      "Collecting coloredlogs\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m219.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sympy in /opt/app-root/lib/python3.9/site-packages (from onnxruntime<2.0.0,>=1.17.0->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.12)\n",
      "Requirement already satisfied: protobuf in /opt/app-root/lib/python3.9/site-packages (from onnxruntime<2.0.0,>=1.17.0->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (3.20.2)\n",
      "Collecting flatbuffers\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->llama-index-core<0.11.0,>=0.10.14->llama-index) (1.16.0)\n",
      "Collecting huggingface_hub<1.0,>=0.16.4\n",
      "  Downloading huggingface_hub-0.21.3-py3-none-any.whl (346 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.2/346.2 kB\u001b[0m \u001b[31m315.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyproject_hooks\n",
      "  Downloading pyproject_hooks-1.0.0-py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /opt/app-root/lib/python3.9/site-packages (from build>=1.0.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (2.0.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in /opt/app-root/lib/python3.9/site-packages (from build>=1.0.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (7.0.1)\n",
      "Collecting starlette<0.37.0,>=0.36.3\n",
      "  Downloading starlette-0.36.3-py3-none-any.whl (71 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m252.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/app-root/lib/python3.9/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers<0.16.0,>=0.15.1->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (3.13.1)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /opt/app-root/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (3.2.2)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/app-root/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.7.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/app-root/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.3.1)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /opt/app-root/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (2.27.0)\n",
      "Collecting importlib-metadata>=4.6\n",
      "  Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
      "Collecting opentelemetry-proto==1.23.0\n",
      "  Downloading opentelemetry_proto-1.23.0-py3-none-any.whl (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m242.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-common==1.23.0\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.23.0-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /opt/app-root/lib/python3.9/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.62.0)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.44b0\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.44b0-py3-none-any.whl (14 kB)\n",
      "Collecting opentelemetry-instrumentation==0.44b0\n",
      "  Downloading opentelemetry_instrumentation-0.44b0-py3-none-any.whl (28 kB)\n",
      "Collecting opentelemetry-util-http==0.44b0\n",
      "  Downloading opentelemetry_util_http-0.44b0-py3-none-any.whl (6.9 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.44b0\n",
      "  Downloading opentelemetry_semantic_conventions-0.44b0-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: setuptools>=16.0 in /opt/app-root/lib/python3.9/site-packages (from opentelemetry-instrumentation==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (68.1.2)\n",
      "Collecting asgiref~=3.0\n",
      "  Downloading asgiref-3.7.2-py3-none-any.whl (24 kB)\n",
      "Collecting backoff>=1.10.0\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Collecting monotonic>=1.5\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Collecting httptools>=0.5.0\n",
      "  Downloading httptools-0.6.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (345 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.2/345.2 kB\u001b[0m \u001b[31m318.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting python-dotenv>=0.13\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Collecting websockets>=10.4\n",
      "  Downloading websockets-12.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.0/130.0 kB\u001b[0m \u001b[31m265.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting uvloop!=0.15.0,!=0.15.1,>=0.14.0\n",
      "  Downloading uvloop-0.19.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m109.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting watchfiles>=0.13\n",
      "  Downloading watchfiles-0.21.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m320.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting humanfriendly>=9.1\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m250.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: zipp>=3.1.0 in /opt/app-root/lib/python3.9/site-packages (from importlib-resources->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (3.17.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/app-root/lib/python3.9/site-packages (from sympy->onnxruntime<2.0.0,>=1.17.0->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (1.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/app-root/lib/python3.9/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/app-root/lib/python3.9/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.3.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/app-root/lib/python3.9/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (5.3.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/app-root/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index) (0.5.1)\n",
      "Building wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=94cc417fa58917c0084474553cdf83a6d76736631f362020d913d880a6e97930\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-frk35t1t/wheels/f7/02/64/d541eac67ec459309d1fb19e727f58ecf7ffb4a8bf42d4cfe5\n",
      "Successfully built pypika\n",
      "Installing collected packages: pypika, monotonic, mmh3, flatbuffers, websockets, uvloop, uvicorn, python-dotenv, pyproject_hooks, pypdf, PyMuPDFb, pulsar-client, orjson, opentelemetry-util-http, opentelemetry-semantic-conventions, opentelemetry-proto, importlib-resources, importlib-metadata, humanfriendly, httptools, chroma-hnswlib, backoff, asgiref, watchfiles, starlette, pymupdf, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, huggingface_hub, coloredlogs, build, bs4, tokenizers, opentelemetry-sdk, opentelemetry-instrumentation, onnxruntime, kubernetes, fastapi, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, llama-index-legacy, opentelemetry-instrumentation-fastapi, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-agent-openai, chromadb, llama-index-vector-stores-chroma, llama-index-program-openai, llama-index-question-gen-openai, llama-index-cli, llama-index\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 7.0.1\n",
      "    Uninstalling importlib-metadata-7.0.1:\n",
      "      Successfully uninstalled importlib-metadata-7.0.1\n",
      "  Attempting uninstall: kubernetes\n",
      "    Found existing installation: kubernetes 25.3.0\n",
      "    Uninstalling kubernetes-25.3.0:\n",
      "      Successfully uninstalled kubernetes-25.3.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "kfp 1.8.22 requires kubernetes<26,>=8.0.0, but you have kubernetes 29.0.0 which is incompatible.\n",
      "codeflare-sdk 0.12.1 requires kubernetes<27,>=25.3.0, but you have kubernetes 29.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed PyMuPDFb-1.23.22 asgiref-3.7.2 backoff-2.2.1 bs4-0.0.2 build-1.1.1 chroma-hnswlib-0.7.3 chromadb-0.4.24 coloredlogs-15.0.1 fastapi-0.110.0 flatbuffers-23.5.26 httptools-0.6.1 huggingface_hub-0.21.3 humanfriendly-10.0 importlib-metadata-6.11.0 importlib-resources-6.1.2 kubernetes-29.0.0 llama-index-0.10.14 llama-index-agent-openai-0.1.5 llama-index-cli-0.1.6 llama-index-embeddings-openai-0.1.6 llama-index-indices-managed-llama-cloud-0.1.3 llama-index-legacy-0.9.48 llama-index-llms-openai-0.1.7 llama-index-multi-modal-llms-openai-0.1.4 llama-index-program-openai-0.1.4 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.6 llama-index-readers-llama-parse-0.1.3 llama-index-vector-stores-chroma-0.1.5 llama-parse-0.3.5 mmh3-4.1.0 monotonic-1.6 onnxruntime-1.17.1 opentelemetry-api-1.23.0 opentelemetry-exporter-otlp-proto-common-1.23.0 opentelemetry-exporter-otlp-proto-grpc-1.23.0 opentelemetry-instrumentation-0.44b0 opentelemetry-instrumentation-asgi-0.44b0 opentelemetry-instrumentation-fastapi-0.44b0 opentelemetry-proto-1.23.0 opentelemetry-sdk-1.23.0 opentelemetry-semantic-conventions-0.44b0 opentelemetry-util-http-0.44b0 orjson-3.9.15 posthog-3.4.2 pulsar-client-3.4.0 pymupdf-1.23.26 pypdf-4.0.2 pypika-0.48.9 pyproject_hooks-1.0.0 python-dotenv-1.0.1 starlette-0.36.3 tokenizers-0.15.2 uvicorn-0.27.1 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47264e32",
   "metadata": {
    "id": "47264e32",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import textwrap\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# stop huggingface warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Uncomment to see debug logs\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Document\n",
    "from llama_index.vector_stores.redis import RedisVectorStore\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c692310",
   "metadata": {
    "id": "3c692310"
   },
   "source": [
    "### Start Redis\n",
    "\n",
    "The easiest way to start Redis as a vector database is using the [redis-stack](https://hub.docker.com/r/redis/redis-stack) docker image.\n",
    "\n",
    "To follow every step of this tutorial, launch the image as follows:\n",
    "\n",
    "```bash\n",
    "docker run --name redis-vecdb -d -p 6379:6379 -p 8001:8001 redis/redis-stack:latest\n",
    "```\n",
    "\n",
    "This will also launch the RedisInsight UI on port 8001 which you can view at http://localhost:8001.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b97a89",
   "metadata": {
    "id": "f9b97a89"
   },
   "source": [
    "### Setup OpenAI\n",
    "Lets first begin by adding the openai api key. This will allow us to access openai for embeddings and to use chatgpt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c9f4d21-145a-401e-95ff-ccb259e8ef84",
   "metadata": {
    "id": "0c9f4d21-145a-401e-95ff-ccb259e8ef84",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103ff054",
   "metadata": {
    "id": "103ff054"
   },
   "source": [
    "Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "304ad9d8",
   "metadata": {
    "id": "304ad9d8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-02-29 23:07:06--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 75042 (73K) [text/plain]\n",
      "Saving to: ‘data/paul_graham/paul_graham_essay.txt’\n",
      "\n",
      "data/paul_graham/pa 100%[===================>]  73.28K  --.-KB/s    in 0.002s  \n",
      "\n",
      "2024-02-29 23:07:06 (32.1 MB/s) - ‘data/paul_graham/paul_graham_essay.txt’ saved [75042/75042]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p 'data/paul_graham/'\n",
    "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ff935d",
   "metadata": {
    "id": "59ff935d"
   },
   "source": [
    "### Read in a dataset\n",
    "Here we will use a set of Paul Graham essays to provide the text to turn into embeddings, store in a ``RedisVectorStore`` and query to find context for our LLM QnA loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "68cbd239-880e-41a3-98d8-dbb3fab55431",
   "metadata": {
    "id": "68cbd239-880e-41a3-98d8-dbb3fab55431",
    "outputId": "b2158af0-5fa2-4bcf-d4bc-222625cd0425",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: 08a431f3-7af5-4e52-b417-7bc01a5be348\n"
     ]
    }
   ],
   "source": [
    "# load documents\n",
    "documents = SimpleDirectoryReader(\"./data/paul_graham\").load_data()\n",
    "print(\n",
    "    \"Document ID:\",\n",
    "    documents[0].doc_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce335c9a",
   "metadata": {
    "id": "ce335c9a"
   },
   "source": [
    "You can process your files individually using [SimpleDirectoryReader](/examples/data_connectors/simple_directory_reader.ipynb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f67cb2a",
   "metadata": {
    "id": "1f67cb2a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/app-root/src/data/paul_graham/paul_graham_essay.txt\n"
     ]
    }
   ],
   "source": [
    "loader = SimpleDirectoryReader(\"./data/paul_graham\")\n",
    "documents = loader.load_data()\n",
    "for doc in list(documents):\n",
    "    for key in list(doc.metadata.keys()):\n",
    "        if doc.metadata[key] is None:\n",
    "            doc.metadata.pop(key)\n",
    "for file in loader.input_files:\n",
    "    print(file)\n",
    "    # Here is where you would do any preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d22e78f-57c2-4bc0-b6a9-7125f10c0e0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd270925",
   "metadata": {
    "id": "dd270925"
   },
   "source": [
    "### Initialize the Redis Vector Store\n",
    "\n",
    "Now we have our documents read in, we can initialize the Redis Vector Store. This will allow us to store our vectors in Redis and create an index.\n",
    "\n",
    "Below you can see the docstring for `RedisVectorStore`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6d7f6c6-805b-4a2a-a9d3-a7c8c9de37ac",
   "metadata": {
    "id": "f6d7f6c6-805b-4a2a-a9d3-a7c8c9de37ac",
    "outputId": "296e6461-0e13-493d-f49f-6a0c379ecfcc",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize RedisVectorStore.\n",
      "\n",
      "        For index arguments that can be passed to RediSearch, see\n",
      "        https://redis.io/docs/stack/search/reference/vectors/\n",
      "\n",
      "        The index arguments will depend on the index type chosen. There\n",
      "        are two available index types\n",
      "            - FLAT: a flat index that uses brute force search\n",
      "            - HNSW: a hierarchical navigable small world graph index\n",
      "\n",
      "        Args:\n",
      "            index_name (str): Name of the index.\n",
      "            index_prefix (str): Prefix for the index. Defaults to \"llama_index\".\n",
      "                The actual prefix used by Redis will be\n",
      "                \"{index_prefix}{prefix_ending}\".\n",
      "            prefix_ending (str): Prefix ending for the index. Be careful when\n",
      "                changing this: https://github.com/jerryjliu/llama_index/pull/6665.\n",
      "                Defaults to \"/vector\".\n",
      "            index_args (Dict[str, Any]): Arguments for the index. Defaults to None.\n",
      "            metadata_fields (List[str]): List of metadata fields to store in the index\n",
      "                (only supports TAG fields).\n",
      "            redis_url (str): URL for the redis instance.\n",
      "                Defaults to \"redis://localhost:6379\".\n",
      "            overwrite (bool): Whether to overwrite the index if it already exists.\n",
      "                Defaults to False.\n",
      "            kwargs (Any): Additional arguments to pass to the redis client.\n",
      "\n",
      "        Raises:\n",
      "            ValueError: If redis-py is not installed\n",
      "            ValueError: If RediSearch is not installed\n",
      "\n",
      "        Examples:\n",
      "            >>> from llama_index.core.vector_stores.redis import RedisVectorStore\n",
      "            >>> # Create a RedisVectorStore\n",
      "            >>> vector_store = RedisVectorStore(\n",
      "            >>>     index_name=\"my_index\",\n",
      "            >>>     index_prefix=\"llama_index\",\n",
      "            >>>     index_args={\"algorithm\": \"HNSW\", \"m\": 16, \"ef_construction\": 200,\n",
      "                \"distance_metric\": \"cosine\"},\n",
      "            >>>     redis_url=\"redis://localhost:6379/\",\n",
      "            >>>     overwrite=True)\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "print(RedisVectorStore.__init__.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba1558b3",
   "metadata": {
    "id": "ba1558b3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext\n",
    "\n",
    "vector_store = RedisVectorStore(\n",
    "    index_name=\"pg_essays\",\n",
    "    index_prefix=\"llama\",\n",
    "    redis_url=\"redis://redis-stack.llama.svc.cluster.local:6379\",  # Default\n",
    "    overwrite=True,\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b481dc47",
   "metadata": {
    "id": "b481dc47"
   },
   "source": [
    "With logging on, it prints out the following:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f2c1e1",
   "metadata": {
    "id": "d3f2c1e1"
   },
   "source": [
    "```bash\n",
    "INFO:llama_index.vector_stores.redis:Creating index pg_essays\n",
    "Creating index pg_essays\n",
    "INFO:llama_index.vector_stores.redis:Added 15 documents to index pg_essays\n",
    "Added 15 documents to index pg_essays\n",
    "INFO:llama_index.vector_stores.redis:Saving index to disk in background\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02256f9",
   "metadata": {
    "id": "e02256f9"
   },
   "source": [
    "Now you can browse these index in redis-cli and read/write it as Redis hash. It looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bb439b",
   "metadata": {
    "id": "83bb439b"
   },
   "source": [
    "```bash\n",
    "$ redis-cli\n",
    "127.0.0.1:6379> keys *\n",
    " 1) \"llama/vector_0f125320-f5cf-40c2-8462-aefc7dbff490\"\n",
    " 2) \"llama/vector_bd667698-4311-4a67-bb8b-0397b03ec794\"\n",
    "127.0.0.1:6379> HGETALL \"llama/vector_bd667698-4311-4a67-bb8b-0397b03ec794\"\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405d445f",
   "metadata": {
    "id": "405d445f"
   },
   "source": [
    "### Handle duplicated index\n",
    "\n",
    "Regardless of whether overwrite=True is used in RedisVectorStore(), the process of generating the index and storing data in Redis still takes time. Currently, it is necessary to implement your own logic to manage duplicate indexes. One possible approach is to set a flag in Redis to indicate the readiness of the index. If the flag is set, you can bypass the index generation step and directly load the index from Redis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "218e612a",
   "metadata": {
    "id": "218e612a",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import redis\n",
    "r = redis.Redis(host='redis-stack.llama.svc.cluster.local', port=6379)\n",
    "index_name = \"pg_essays\"\n",
    "r.set(f\"added:{index_name}\", \"true\")\n",
    "\n",
    "# Later in code\n",
    "#if r.get(f\"added:{index_name}\"):\n",
    "    # Skip to deploy your index, restore it. Please see \"Restore index from Redis\" section below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04304299-fc3e-40a0-8600-f50c3292767e",
   "metadata": {
    "id": "04304299-fc3e-40a0-8600-f50c3292767e"
   },
   "source": [
    "### Query the data\n",
    "Now that we have our document stored in the index, we can ask questions against the index. The index will use the data stored in itself as the knowledge base for ChatGPT. The default setting for as_query_engine() utilizes OpenAI embeddings and ChatGPT as the language model. Therefore, an OpenAI key is required unless you opt for a customized or local language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35369eda",
   "metadata": {
    "id": "35369eda",
    "outputId": "eb516c5c-ce16-4b1a-dfb7-f6ecfaecdfbc",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author learned that with microcomputers, there was a significant shift in the way computers\n",
      "could be interacted with, allowing for immediate responses to keystrokes and a more engaging\n",
      "programming experience compared to the older systems that relied on punch cards for input.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author learn?\")\n",
    "print(textwrap.fill(str(response), 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99212d33",
   "metadata": {
    "id": "99212d33",
    "outputId": "1f55e985-9248-40af-daa7-50ab994f2495",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A hard moment for the author was when he learned that it was possible for programs not to terminate\n",
      "while working on the IBM 1401 in 9th grade.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What was a hard moment for the author?\")\n",
    "print(textwrap.fill(str(response), 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7bc976",
   "metadata": {
    "id": "4d7bc976"
   },
   "source": [
    "### Saving and Loading\n",
    "\n",
    "Redis allows the user to perform backups in the background or synchronously. With Llamaindex, the ``RedisVectorStore.persist()`` function can be used to trigger such a backup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09836567",
   "metadata": {
    "id": "09836567",
    "outputId": "9e8a86cb-3d75-4aca-d904-ae9abfacdd07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "redis  redisinsight\n"
     ]
    }
   ],
   "source": [
    "!docker exec -it redis-vecdb ls /data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ef500b",
   "metadata": {
    "id": "93ef500b"
   },
   "outputs": [],
   "source": [
    "# RedisVectorStore's persist method doesn't use the persist_path argument\n",
    "vector_store.persist(persist_path=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5ab256",
   "metadata": {
    "id": "ed5ab256",
    "outputId": "7daa6624-54db-44a2-a702-9f3f179931a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dump.rdb  redis  redisinsight\n"
     ]
    }
   ],
   "source": [
    "!docker exec -it redis-vecdb ls /data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8849ba",
   "metadata": {
    "id": "8c8849ba"
   },
   "source": [
    "### Restore index from Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95817a85",
   "metadata": {
    "id": "95817a85",
    "tags": []
   },
   "outputs": [],
   "source": [
    "vector_store = RedisVectorStore(\n",
    "    index_name=\"pg_essays\",\n",
    "    index_prefix=\"llama\",\n",
    "    redis_url=\"redis://redis-stack.llama.svc.cluster.local:6379\",\n",
    "    overwrite=True,\n",
    ")\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b12c9f2",
   "metadata": {
    "id": "5b12c9f2"
   },
   "source": [
    "Now you can reuse your index as discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "437dc580",
   "metadata": {
    "id": "437dc580",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='d75abbe9-5bd4-427f-996f-f7051ffb4dea', embedding=None, metadata={'file_path': '/opt/app-root/src/data/paul_graham/paul_graham_essay.txt', 'file_name': '/opt/app-root/src/data/paul_graham/paul_graham_essay.txt', 'file_type': 'text/plain', 'file_size': 75042, 'creation_date': '2024-02-29', 'last_modified_date': '2024-02-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='7312dd11-943e-4e17-9d43-9194040c2d8c', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'file_path': '/opt/app-root/src/data/paul_graham/paul_graham_essay.txt', 'file_name': '/opt/app-root/src/data/paul_graham/paul_graham_essay.txt', 'file_type': 'text/plain', 'file_size': 75042, 'creation_date': '2024-02-29', 'last_modified_date': '2024-02-29'}, hash='b4aae386cf74db5e71ce33a8570a4fd780774b93015554c171f0385cc1d90c52'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='725a4230-1e96-4a7b-9a6f-c6ee945e1b25', node_type=<ObjectType.TEXT: '1'>, metadata={'file_path': '/opt/app-root/src/data/paul_graham/paul_graham_essay.txt', 'file_name': '/opt/app-root/src/data/paul_graham/paul_graham_essay.txt', 'file_type': 'text/plain', 'file_size': 75042, 'creation_date': '2024-02-29', 'last_modified_date': '2024-02-29'}, hash='ea06e32647b468a7c3cedcfd7332e82f673b426002aba3c6ebc4458d9f930ba2'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='a0cfe296-6e49-4b79-8387-44cd777dad6b', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='7a2ff7c89fe460b214ceb98f2fa4cd544496dd2649ae5d0bc33a48177f034a56')}, text=\"Now that I could write essays again, I wrote a bunch about topics I'd had stacked up. I kept writing essays through 2020, but I also started to think about other things I could work on. How should I choose what to do? Well, how had I chosen what to work on in the past? I wrote an essay for myself to answer that question, and I was surprised how long and messy the answer turned out to be. If this surprised me, who'd lived it, then I thought perhaps it would be interesting to other people, and encouraging to those with similarly messy lives. So I wrote a more detailed version for others to read, and this is the last sentence of it.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNotes\\n\\n[1] My experience skipped a step in the evolution of computers: time-sharing machines with interactive OSes. I went straight from batch processing to microcomputers, which made microcomputers seem all the more exciting.\\n\\n[2] Italian words for abstract concepts can nearly always be predicted from their English cognates (except for occasional traps like polluzione). It's the everyday words that differ. So if you string together a lot of abstract concepts with a few simple verbs, you can make a little Italian go a long way.\\n\\n[3] I lived at Piazza San Felice 4, so my walk to the Accademia went straight down the spine of old Florence: past the Pitti, across the bridge, past Orsanmichele, between the Duomo and the Baptistery, and then up Via Ricasoli to Piazza San Marco. I saw Florence at street level in every possible condition, from empty dark winter evenings to sweltering summer days when the streets were packed with tourists.\\n\\n[4] You can of course paint people like still lives if you want to, and they're willing. That sort of portrait is arguably the apex of still life painting, though the long sitting does tend to produce pained expressions in the sitters.\\n\\n[5] Interleaf was one of many companies that had smart people and built impressive technology, and yet got crushed by Moore's Law. In the 1990s the exponential growth in the power of commodity (i.e. Intel) processors rolled up high-end, special-purpose hardware and software companies like a bulldozer.\\n\\n[6] The signature style seekers at RISD weren't specifically mercenary. In the art world, money and coolness are tightly coupled. Anything expensive comes to be seen as cool, and anything seen as cool will soon become equally expensive.\\n\\n[7] Technically the apartment wasn't rent-controlled but rent-stabilized, but this is a refinement only New Yorkers would know or care about. The point is that it was really cheap, less than half market price.\\n\\n[8] Most software you can launch as soon as it's done. But when the software is an online store builder and you're hosting the stores, if you don't have any users yet, that fact will be painfully obvious. So before we could launch publicly we had to launch privately, in the sense of recruiting an initial set of users and making sure they had decent-looking stores.\\n\\n[9] We'd had a code editor in Viaweb for users to define their own page styles. They didn't know it, but they were editing Lisp expressions underneath. But this wasn't an app editor, because the code ran when the merchants' sites were generated, not when shoppers visited them.\\n\\n[10] This was the first instance of what is now a familiar experience, and so was what happened next, when I read the comments and found they were full of angry people. How could I claim that Lisp was better than other languages? Weren't they all Turing complete? People who see the responses to essays I write sometimes tell me how sorry they feel for me, but I'm not exaggerating when I reply that it has always been like this, since the very beginning. It comes with the territory. An essay must tell readers things they don't already know, and some people dislike being told such things.\\n\\n[11] People put plenty of stuff on the internet in the 90s of course, but putting something online is not the same as publishing it online. Publishing online means you treat the online version as the (or at least a) primary version.\\n\\n[12] There is a general lesson here that our experience with Y Combinator also teaches: Customs continue to constrain you long after the restrictions that caused them have disappeared. Customary VC practice had once, like the customs about publishing essays, been based on real constraints. Startups had once been much more expensive to start, and proportionally rare.\", start_char_idx=66856, end_char_idx=71281, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.771157622337),\n",
       " NodeWithScore(node=TextNode(id_='e3f32cf7-7563-4e8c-b7d7-467d838bc243', embedding=None, metadata={'file_path': '/opt/app-root/src/data/paul_graham/paul_graham_essay.txt', 'file_name': '/opt/app-root/src/data/paul_graham/paul_graham_essay.txt', 'file_type': 'text/plain', 'file_size': 75042, 'creation_date': '2024-02-29', 'last_modified_date': '2024-02-29'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='7312dd11-943e-4e17-9d43-9194040c2d8c', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'file_path': '/opt/app-root/src/data/paul_graham/paul_graham_essay.txt', 'file_name': '/opt/app-root/src/data/paul_graham/paul_graham_essay.txt', 'file_type': 'text/plain', 'file_size': 75042, 'creation_date': '2024-02-29', 'last_modified_date': '2024-02-29'}, hash='b4aae386cf74db5e71ce33a8570a4fd780774b93015554c171f0385cc1d90c52'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='057640b2-f966-4d37-be21-4150e378115a', node_type=<ObjectType.TEXT: '1'>, metadata={'file_path': '/opt/app-root/src/data/paul_graham/paul_graham_essay.txt', 'file_name': '/opt/app-root/src/data/paul_graham/paul_graham_essay.txt', 'file_type': 'text/plain', 'file_size': 75042, 'creation_date': '2024-02-29', 'last_modified_date': '2024-02-29'}, hash='961986d2ff7ceb67db14258ef61f79a7f8e83de40d362d424e34db6c77618750'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='93c2bd95-3ca5-46b1-bc37-4c982fa0e627', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='deaee6d5c992dbf757876957aa9112a42d30a636c6c83d81fcfac4aaf2d24dee')}, text='And at the same time all involved would adhere outwardly to the conventions of a 19th century atelier. We actually had one of those little stoves, fed with kindling, that you see in 19th century studio paintings, and a nude model sitting as close to it as possible without getting burned. Except hardly anyone else painted her besides me. The rest of the students spent their time chatting or occasionally trying to imitate things they\\'d seen in American art magazines.\\n\\nOur model turned out to live just down the street from me. She made a living from a combination of modelling and making fakes for a local antique dealer. She\\'d copy an obscure old painting out of a book, and then he\\'d take the copy and maltreat it to make it look old. [3]\\n\\nWhile I was a student at the Accademia I started painting still lives in my bedroom at night. These paintings were tiny, because the room was, and because I painted them on leftover scraps of canvas, which was all I could afford at the time. Painting still lives is different from painting people, because the subject, as its name suggests, can\\'t move. People can\\'t sit for more than about 15 minutes at a time, and when they do they don\\'t sit very still. So the traditional m.o. for painting people is to know how to paint a generic person, which you then modify to match the specific person you\\'re painting. Whereas a still life you can, if you want, copy pixel by pixel from what you\\'re seeing. You don\\'t want to stop there, of course, or you get merely photographic accuracy, and what makes a still life interesting is that it\\'s been through a head. You want to emphasize the visual cues that tell you, for example, that the reason the color changes suddenly at a certain point is that it\\'s the edge of an object. By subtly emphasizing such things you can make paintings that are more realistic than photographs not just in some metaphorical sense, but in the strict information-theoretic sense. [4]\\n\\nI liked painting still lives because I was curious about what I was seeing. In everyday life, we aren\\'t consciously aware of much we\\'re seeing. Most visual perception is handled by low-level processes that merely tell your brain \"that\\'s a water droplet\" without telling you details like where the lightest and darkest points are, or \"that\\'s a bush\" without telling you the shape and position of every leaf. This is a feature of brains, not a bug. In everyday life it would be distracting to notice every leaf on every bush. But when you have to paint something, you have to look more closely, and when you do there\\'s a lot to see. You can still be noticing new things after days of trying to paint something people usually take for granted, just as you can after days of trying to write an essay about something people usually take for granted.\\n\\nThis is not the only way to paint. I\\'m not 100% sure it\\'s even a good way to paint. But it seemed a good enough bet to be worth trying.\\n\\nOur teacher, professor Ulivi, was a nice guy. He could see I worked hard, and gave me a good grade, which he wrote down in a sort of passport each student had. But the Accademia wasn\\'t teaching me anything except Italian, and my money was running out, so at the end of the first year I went back to the US.\\n\\nI wanted to go back to RISD, but I was now broke and RISD was very expensive, so I decided to get a job for a year and then return to RISD the next fall. I got one at a company called Interleaf, which made software for creating documents. You mean like Microsoft Word? Exactly. That was how I learned that low end software tends to eat high end software. But Interleaf still had a few years to live yet. [5]\\n\\nInterleaf had done something pretty bold. Inspired by Emacs, they\\'d added a scripting language, and even made the scripting language a dialect of Lisp. Now they wanted a Lisp hacker to write things in it. This was the closest thing I\\'ve had to a normal job, and I hereby apologize to my boss and coworkers, because I was a bad employee. Their Lisp was the thinnest icing on a giant C cake, and since I didn\\'t know C and didn\\'t want to learn it, I never understood most of the software. Plus I was terribly irresponsible. This was back when a programming job meant showing up every day during certain working hours.', start_char_idx=14179, end_char_idx=18443, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.758605718613)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pgQuery = index.as_query_engine()\n",
    "pgQuery.query(\"What is the meaning of life?\")\n",
    "# or\n",
    "pgRetriever = index.as_retriever()\n",
    "pgRetriever.retrieve(\"What is the meaning of life?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43b185f",
   "metadata": {
    "id": "e43b185f"
   },
   "source": [
    "Learn more about [query_engine](/module_guides/deploying/query_engine/root.md)  and [retrievers](/module_guides/querying/retriever/root.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b975a7",
   "metadata": {
    "id": "52b975a7"
   },
   "source": [
    "### Deleting documents or index completely\n",
    "\n",
    "Sometimes it may be useful to delete documents or the entire index. This can be done using the `delete` and `delete_index` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe322f7",
   "metadata": {
    "id": "6fe322f7",
    "outputId": "164886f9-9f26-49a2-8644-fe7742c49b21"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'faa23c94-ac9e-4763-92ba-e0f87bf38195'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_id = documents[0].doc_id\n",
    "document_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4fb2b0",
   "metadata": {
    "id": "ae4fb2b0",
    "outputId": "edca91e4-00b3-4824-86eb-f483a281f477"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents 20\n"
     ]
    }
   ],
   "source": [
    "redis_client = vector_store.client\n",
    "print(\"Number of documents\", len(redis_client.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce45788",
   "metadata": {
    "id": "0ce45788"
   },
   "outputs": [],
   "source": [
    "vector_store.delete(document_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1ac683",
   "metadata": {
    "id": "4a1ac683",
    "outputId": "1f56ab21-a9e8-437a-9f90-5cd6e489951c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents 10\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents\", len(redis_client.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c380605a",
   "metadata": {
    "id": "c380605a"
   },
   "outputs": [],
   "source": [
    "# now lets delete the index entirely (happens in the background, may take a second)\n",
    "# this will delete all the documents and the index\n",
    "vector_store.delete_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474ad4ee",
   "metadata": {
    "id": "474ad4ee",
    "outputId": "7fbaa057-10ba-451d-a8ce-c9612da42dcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents\", len(redis_client.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b67496",
   "metadata": {
    "id": "61b67496"
   },
   "source": [
    "### Working with Metadata\n",
    "\n",
    "RedisVectorStore supports adding metadata and then using it in your queries (for example, to limit the scope of documents retrieved). However, there are a couple of important caveats:\n",
    "1. Currently, only [Tag fields](https://redis.io/docs/stack/search/reference/tags/) are supported, and only with exact match.\n",
    "2. You must declare the metadata when creating the index (usually when initializing RedisVectorStore). If you do not do this, your queries will come back empty. There is no way to modify an existing index after it had already been created (this is a Redis limitation).\n",
    "\n",
    "Here's how to work with Metadata:\n",
    "\n",
    "\n",
    "### When **creating** the index\n",
    "\n",
    "Make sure to declare the metadata when you **first** create the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9889ec79",
   "metadata": {
    "id": "9889ec79",
    "tags": []
   },
   "outputs": [],
   "source": [
    "vector_store = RedisVectorStore(\n",
    "    index_name=\"pg_essays_with_metadata\",\n",
    "    index_prefix=\"llama\",\n",
    "    redis_url=\"redis://redis-stack.llama.svc.cluster.local:6379\",\n",
    "    overwrite=True,\n",
    "    metadata_fields=[\"user_id\", \"favorite_color\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d6dc21",
   "metadata": {
    "id": "f8d6dc21"
   },
   "source": [
    "Note: the field names `text`, `doc_id`, `id` and the name of your vector field (`vector` by default) should **not** be used as metadata field names, as they are are reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429947d5",
   "metadata": {
    "id": "429947d5"
   },
   "source": [
    "### When adding a document\n",
    "\n",
    "Add your metadata under the `metadata` key. You can add metadata to documents you load in just by looping over them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89781b7d",
   "metadata": {
    "id": "89781b7d",
    "outputId": "325b55c6-0be2-40aa-b3d8-2c871cfe0b97",
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Document' object has no attribute 'doc_hash'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 17\u001b[0m\n\u001b[1;32m      8\u001b[0m index \u001b[38;5;241m=\u001b[39m VectorStoreIndex\u001b[38;5;241m.\u001b[39mfrom_documents(\n\u001b[1;32m      9\u001b[0m     documents, storage_context\u001b[38;5;241m=\u001b[39mstorage_context\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# load documents\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocument ID:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m     documents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdoc_id,\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocument Hash:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m---> 17\u001b[0m     \u001b[43mdocuments\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdoc_hash\u001b[49m,\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMetadata:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     19\u001b[0m     documents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmetadata,\n\u001b[1;32m     20\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Document' object has no attribute 'doc_hash'"
     ]
    }
   ],
   "source": [
    "# load your documents normally, then add your metadata\n",
    "documents = SimpleDirectoryReader(\"./data/paul_graham\").load_data()\n",
    "\n",
    "for document in documents:\n",
    "    document.metadata = {\"user_id\": \"12345\", \"favorite_color\": \"blue\"}\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context\n",
    ")\n",
    "\n",
    "# load documents\n",
    "print(\n",
    "    \"Document ID:\",\n",
    "    documents[0].doc_id,\n",
    "    \"Document Hash:\",\n",
    "    documents[0].doc_hash,\n",
    "    \"Metadata:\",\n",
    "    documents[0].metadata,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b24e76",
   "metadata": {
    "id": "42b24e76"
   },
   "source": [
    "### When querying the index\n",
    "\n",
    "To filter by your metadata fields, include one or more of your metadata keys, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b01f346",
   "metadata": {
    "id": "0b01f346",
    "outputId": "77e9558d-dd98-412a-9d74-d820009d3567",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author learned that the traditional approach to artificial intelligence, which involved explicit\n",
      "data structures representing concepts, was not effective in achieving true understanding of natural\n",
      "language. This realization led the author to focus on Lisp and delve into writing a book about Lisp\n",
      "hacking, which helped them learn more about the language.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.vector_stores import MetadataFilters, ExactMatchFilter\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=3,\n",
    "    filters=MetadataFilters(\n",
    "        filters=[\n",
    "            ExactMatchFilter(key=\"user_id\", value=\"12345\"),\n",
    "            ExactMatchFilter(key=\"favorite_color\", value=\"blue\"),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What did the author learn?\")\n",
    "print(textwrap.fill(str(response), 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07514f85",
   "metadata": {
    "id": "07514f85"
   },
   "source": [
    "## Troubleshooting\n",
    "\n",
    "In case you run into issues retrieving your documents from the index, you might get a message similar to this.\n",
    "```\n",
    "No docs found on index 'pg_essays' with prefix 'llama' and filters '(@user_id:{12345} & @favorite_color:{blue})'.\n",
    "* Did you originally create the index with a different prefix?\n",
    "* Did you index your metadata fields when you created the index?\n",
    "```\n",
    "\n",
    "If you get this error, there a couple of gotchas to be aware of when working with Redis:\n",
    "#### Prefix issues\n",
    "\n",
    "If you first create your index with a specific `prefix` but later change that prefix in your code, your query will come back empty. Redis saves the prefix your originally created your index with and expects it to be consistent.\n",
    "\n",
    "To see what prefix your index was created with, you can run `FT.INFO <name of your index>` in the Redis CLI and look under `index_definition` => `prefixes`.\n",
    "\n",
    "#### Empty queries when using metadata\n",
    "\n",
    "If you add metadata to the index *after* it has already been created and then try to query over that metadata, your queries will come back empty.\n",
    "\n",
    "Redis indexes fields upon index creation only (similar to how it indexes the prefixes, above).\n",
    "\n",
    "If you have an existing index and want to make sure it's dropped, you can run `FT.DROPINDEX <name of your index>` in the Redis CLI. Note that this will *not* drop your actual data."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
